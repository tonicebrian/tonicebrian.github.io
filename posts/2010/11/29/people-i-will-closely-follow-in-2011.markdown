---
comments: true
date: 2010-11-29 11:00:32
layout: post
slug: people-i-will-closely-follow-in-2011
title: People I will closely follow in 2011
wordpressid: 130
tags: people,research
---

No one works in isolation. Everybody is looking for inspiration, ideas or exemplifying careers in his field. Specially in the technological field, _we stand on the shoulders of giants_. Those who follow are my giants for the next year, they are programmers, researchers or professors and they are working on one field of interest to me. Here they are:

[![](lemire.png)](http://lemire.net)

[Daniel Lemire](http://lemire.me/blog/) is a Canadian university professor working in the field of Recommender Systems. I first heard about him when researching high performance recommender systems I stumbled upon his simple yet effective [Slope One](http://en.wikipedia.org/wiki/Slope_One) recommender algorithm. However, lately he has been actively discussing in his blog about the inner workings of the scientific community, the peer review process and the validity of the University model as the best way to disseminate scientific knowledge in an age with zero costs for accessing information. I find those posts very stimulating.

[![](DavidMacKay.png)](http://www.inference.phy.cam.ac.uk/mackay/)

[David MacKay](http://www.inference.phy.cam.ac.uk/mackay/) is a professor at the Inference Group in the University of Cambridge. His book about [Information Theory](http://www.inference.phy.cam.ac.uk/mackay/itila/) is one of the best written ones I have read. It exposed me to [Gaussian Processes](http://en.wikipedia.org/wiki/Gaussian_process) in a clear and understandable way for the first time. But not only for his research quality, David is remarkable because is a professor that does not live in his ivory tower. He is able to apply his research to help the impaired with the [Dasher project](http://www.inference.phy.cam.ac.uk/dasher/). Data mining and information theory for the greater good. And as an extra bonus his interest on tackling the global warming from a [scientific point of view](http://www.withouthotair.com/) is very inspiring and necessary in an age when corporations dictate agenda. And all his publications freely accessible to everyone.

[![](jordan.png)](http://www.cs.berkeley.edu/~jordan/)

[Michael I. Jordan](http://www.cs.berkeley.edu/~jordan/) is a difficult search term in the Internet :). He is a professor in Berkeley working on machine learning and one of the first proponents of [Bayesian Networks](http://www.amazon.com/gp/product/0262600323?ie=UTF8&tag=tonicebrianco-20&linkCode=as2&camp=1789&creative=390957&creativeASIN=0262600323)![](http://www.assoc-amazon.com/e/ir?t=tonicebrianco-20&l=as2&o=1&a=0262600323) as learning models. Although I don't know his work very well, a friend, whose opinion I value a lot, recommended him as the _Midas of research_. Everything he publishes becomes a hit after a couple of years. So a sure bet to ride.

[](http://measuringmeasures.com/)

![](bradford.png)

[Bradford Cross](http://measuringmeasures.com/) is the only non academic in this list. He is co-founder and head of research of [FlightCaster](http://flightcaster.com/), a small startup that predicts flight delays using Statistical Learning techniques. He presents the right balance between engineering and research that appeals to me the most. He can write posts about using Clojure for [Mining the web](http://measuringmeasures.com/blog/2010/11/8/mining-the-web-with-clojure.html) or using it for managing [Hadoop and Cascade instances](http://measuringmeasures.com/blog/2009/12/16/flightcaster-supports-clojure.html) or write excellent [posts](http://measuringmeasures.com/blog/2010/3/12/learning-about-machine-learning-2nd-ed.html) about the best way to self-learn Statistical Learning Theory.

[](http://yann.lecun.com/)

[![](ylc.png)](http://yann.lecun.com/)

[Yann LeCun](http://yann.lecun.com/) and [Geoffrey Hinton](http://www.cs.toronto.edu/~hinton/) are two researchers that started in the early 90's doing work on Neural Networks.  If you have taken any course on Neural Networks you probably have seen mentioned the 10 digit character recognition dataset and how NN are used for learning it. This dataset was compiled by LeCun. In this decade, they have realized the limitations of the Neural Network architectures and they are now working in a new area of Machine Learning called [Deep Learning](http://deeplearning.net/). In deep learning architectures, the learning agents are chained in layers that work with low level data to provide information to upper layers, which in time are using this information as low level input. The ambitious goal is to generate true Artificial Intelligence, so worth checking if they succeed or not.

[![](http://www.tonicebrian.com/blog/wp-content/uploads/2010/11/juergen2010genova110+58-150x110.gif)](http://www.tonicebrian.com/blog/wp-content/uploads/2010/11/juergen2010genova110+58.gif)

[![](http://www.tonicebrian.com/blog/wp-content/uploads/2010/11/MarcusHutter-150x150.jpg)](http://www.hutter1.net/)

[Jürgen Schmidhuber](http://www.idsia.ch/~juergen/) and [Marcus Hutter](http://www.hutter1.net/). Hutter was working under the supervision of the Schmidhuber in a [Theory of Universal Learning Machines](http://www.idsia.ch/~juergen/unilearn.html) that tries to unify Algorithmic Learning Theory with Reinforcement Learning. As the name points, this theory aims at building a universal problem solver and thus true AI. [Marcus' book](http://www.amazon.com/gp/redirect.html?ie=UTF8&location=http%3A%2F%2Fwww.amazon.com%2Fs%3Fie%3DUTF8%26tag%3Dwwwcanoniccom-20%26index%3Dblended%26link_code%3Dqs%26field-keywords%3Duniversal%2520artificial%2520intelligence%26sourceid%3DMozilla-search&tag=tonicebrianco-20&linkCode=ur2&camp=1789&creative=390957)![](https://www.assoc-amazon.com/e/ir?t=tonicebrianco-20&l=ur2&o=1) has been sitting in my reading pile for a long time. It is a difficult read because the book rapidly moves into layers of abstraction when it is supposed to be explaining learning algorithms, but anyway I feel a strange attraction to this book and I will surely read it in the future.

So this is my Hall of Fame for 2011. Lots of reading and learning.
